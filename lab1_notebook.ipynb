{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running Pyspark in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\ekane\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\ekane\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\ekane\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\ekane\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\ekane\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\ekane\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\ekane\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# spark-3.2.2-bin-hadoop3.2\n",
    "# install findspark using pip\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis and Regression on Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the boston housing dataset\n",
    "dataset = spark.read.csv('data/BostonHousing.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Exploration : Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|          Attributes|medv|\n",
      "+--------------------+----+\n",
      "|[0.00632,18.0,2.3...|24.0|\n",
      "|[0.02731,0.0,7.07...|21.6|\n",
      "|[0.02729,0.0,7.07...|34.7|\n",
      "|[0.03237,0.0,2.18...|33.4|\n",
      "|[0.06905,0.0,2.18...|36.2|\n",
      "|[0.02985,0.0,2.18...|28.7|\n",
      "|[0.08829,12.5,7.8...|22.9|\n",
      "|[0.14455,12.5,7.8...|27.1|\n",
      "|[0.21124,12.5,7.8...|16.5|\n",
      "|[0.17004,12.5,7.8...|18.9|\n",
      "|[0.22489,12.5,7.8...|15.0|\n",
      "|[0.11747,12.5,7.8...|18.9|\n",
      "|[0.09378,12.5,7.8...|21.7|\n",
      "|[0.62976,0.0,8.14...|20.4|\n",
      "|[0.63796,0.0,8.14...|18.2|\n",
      "|[0.62739,0.0,8.14...|19.9|\n",
      "|[1.05393,0.0,8.14...|23.1|\n",
      "|[0.7842,0.0,8.14,...|17.5|\n",
      "|[0.80271,0.0,8.14...|20.2|\n",
      "|[0.7258,0.0,8.14,...|18.2|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "# Convert all the features from different columns into a single column\n",
    "# Let's call this new vector column as 'Attributes' in the outputCol parameter\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Input all the features in one vector column\n",
    "assembler = VectorAssembler(inputCols=['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'], outputCol='Attributes')\n",
    "\n",
    "# Use the assembler object to transform the dataset\n",
    "output = assembler.transform(dataset)\n",
    "\n",
    "# Input vs Output\n",
    "finalized_data = output.select('Attributes', 'medv')\n",
    "\n",
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing data\n",
    "train_data, test_data = finalized_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Learn and predict the Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EKANE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|          Attributes|medv|        prediction|\n",
      "+--------------------+----+------------------+\n",
      "|[0.01381,80.0,0.4...|50.0| 40.99234580781855|\n",
      "|[0.01439,60.0,2.9...|29.1|31.718477818040853|\n",
      "|[0.01709,90.0,2.0...|30.1|25.166621887817712|\n",
      "|[0.01951,17.5,1.3...|33.0| 23.13869167935512|\n",
      "|[0.02543,55.0,3.7...|23.9| 27.93171690439138|\n",
      "|[0.02731,0.0,7.07...|21.6| 25.28478869656535|\n",
      "|[0.02985,0.0,2.18...|28.7| 25.19280558015525|\n",
      "|[0.03041,0.0,5.19...|18.5| 19.01263292794857|\n",
      "|[0.03237,0.0,2.18...|33.4| 28.41169475012503|\n",
      "|[0.03466,35.0,6.0...|19.4|23.342511801554636|\n",
      "|[0.03502,80.0,4.9...|28.5| 33.77173172342891|\n",
      "|[0.03578,20.0,3.3...|45.4| 38.97604974116794|\n",
      "|[0.03871,52.5,5.3...|23.2|27.178636942880445|\n",
      "|[0.03932,0.0,3.41...|22.0|27.660238991915033|\n",
      "|[0.04203,28.0,15....|22.9|29.431010977827448|\n",
      "|[0.04294,28.0,15....|20.6| 27.72804839897894|\n",
      "|[0.04666,80.0,1.5...|30.3|32.903768278630196|\n",
      "|[0.05023,35.0,6.0...|17.1|  19.9621106227574|\n",
      "|[0.05302,0.0,3.41...|28.7|30.936343094784895|\n",
      "|[0.0536,21.0,5.64...|25.0|27.444162931696855|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression(featuresCol='Attributes', labelCol='medv')\n",
    "\n",
    "# Fit the model from the training data\n",
    "regressor = regressor.fit(train_data)\n",
    "\n",
    "# Predict the output for the test data\n",
    "pred_results = regressor.evaluate(test_data)\n",
    "\n",
    "# Predict the model\n",
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Print the coefficients and intercept for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.10811352639957023,0.05106316232531196,0.04617665449513621,3.2782831464761752,-21.530120251995523,3.7037096720122347,0.010525252750265073,-1.5548193917372728,0.2924655132960359,-0.011137946309134606,-0.9644053882628786,0.009353938462252858,-0.549101222121598]\n",
      "Intercept: 38.75295476840933\n"
     ]
    }
   ],
   "source": [
    "# Coefficient of the regression model\n",
    "coeff = regressor.coefficients\n",
    "\n",
    "# X and Y intercept\n",
    "intercept = regressor.intercept\n",
    "\n",
    "print('Coefficients: %s' % str(coeff))\n",
    "print('Intercept: %s' % str(intercept))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE) on test data = 4.43927\n",
      "Root Mean Square Error:  4.43927179267644\n",
      "Mean Absolute Error:  3.469671474744002\n",
      "R2:  0.7367837381989424\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "eval = RegressionEvaluator(labelCol='medv', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "# Root Mean Square Error\n",
    "rmse = eval.evaluate(pred_results.predictions, {eval.metricName: \"rmse\"})\n",
    "print(\"Root Mean Square Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "# Mean Square Error\n",
    "mse = eval.evaluate(pred_results.predictions, {eval.metricName: \"mse\"})\n",
    "print('Root Mean Square Error: ', rmse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = eval.evaluate(pred_results.predictions, {eval.metricName: \"mae\"})\n",
    "print('Mean Absolute Error: ', mae)\n",
    "\n",
    "# R2\n",
    "r2 = eval.evaluate(pred_results.predictions, {eval.metricName: \"r2\"})\n",
    "print('R2: ', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Clustering the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----------+\n",
      "|          Attributes|medv|prediction|\n",
      "+--------------------+----+----------+\n",
      "|[0.00632,18.0,2.3...|24.0|         0|\n",
      "|[0.02731,0.0,7.07...|21.6|         0|\n",
      "|[0.02729,0.0,7.07...|34.7|         0|\n",
      "|[0.03237,0.0,2.18...|33.4|         0|\n",
      "|[0.06905,0.0,2.18...|36.2|         0|\n",
      "|[0.02985,0.0,2.18...|28.7|         0|\n",
      "|[0.08829,12.5,7.8...|22.9|         0|\n",
      "|[0.14455,12.5,7.8...|27.1|         0|\n",
      "|[0.21124,12.5,7.8...|16.5|         0|\n",
      "|[0.17004,12.5,7.8...|18.9|         0|\n",
      "|[0.22489,12.5,7.8...|15.0|         0|\n",
      "|[0.11747,12.5,7.8...|18.9|         0|\n",
      "|[0.09378,12.5,7.8...|21.7|         0|\n",
      "|[0.62976,0.0,8.14...|20.4|         0|\n",
      "|[0.63796,0.0,8.14...|18.2|         0|\n",
      "|[0.62739,0.0,8.14...|19.9|         0|\n",
      "|[1.05393,0.0,8.14...|23.1|         0|\n",
      "|[0.7842,0.0,8.14,...|17.5|         0|\n",
      "|[0.80271,0.0,8.14...|20.2|         0|\n",
      "|[0.7258,0.0,8.14,...|18.2|         0|\n",
      "+--------------------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans(featuresCol='Attributes').setK(2).setSeed(1)\n",
    "model = kmeans.fit(finalized_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(finalized_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Churn analysis in Spark\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b00dfcb09fda5940c949d1804563cdd8ad6cb62ea2cfdf21262c34d902518a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
